{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "143ae6b8",
      "metadata": {},
      "source": [
        "# Combined Electricity & Weather EDA\n",
        "\n",
        "This notebook consolidates the global exploratory analysis from `data_loader.py` with the per-meter tooling from `per_meter_eda.py`, and extends it with intra-meter weekly diagnostics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "783ee05a",
      "metadata": {},
      "source": [
        "## Notebook Guide\n",
        "\n",
        "- Configure `DATA_PATH` below to point at the merged electricity/weather dataset.\n",
        "- Helper utilities keep track of column naming differences (upper vs lower case).\n",
        "- Run the notebook sequentially so that downstream cells reuse shared tables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d0e4b0d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from pandas.api.types import DatetimeTZDtype\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Dict, Iterable, List, Optional\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    import seaborn as sns\n",
        "except ImportError:\n",
        "    sns = None\n",
        "\n",
        "try:\n",
        "    from IPython.display import display\n",
        "except ImportError:\n",
        "    display = print  # type: ignore[arg-type]\n",
        "\n",
        "if sns is not None:\n",
        "    sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
        "plt.rcParams[\"axes.titlesize\"] = 14\n",
        "plt.rcParams[\"axes.labelsize\"] = 12\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.precision\", 4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0b6dec5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Set DATA_PATH to the merged electricity/weather dataset before running the next cell.\n"
          ]
        }
      ],
      "source": [
        "# Update this list if your dataset lives elsewhere.\n",
        "DEFAULT_DATA_PATHS = [\n",
        "    Path(\"/Users/jovanpopovic/Documents/Uni/Honours 2025/honours_project/datasets/original_data.csv\"),\n",
        "]\n",
        "\n",
        "DATA_PATH = next((path for path in DEFAULT_DATA_PATHS if path.exists()), None)\n",
        "\n",
        "# if DATA_PATH is None:\n",
        "#     DATA_PATH = Path(\"/absolute/path/to/merged_electricity_weather.parquet\")\n",
        "#     print(\"Set DATA_PATH to the merged electricity/weather dataset before running the next cell.\")\n",
        "# else:\n",
        "#     print(f\"Using dataset at: {DATA_PATH.resolve()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "150a7996",
      "metadata": {},
      "outputs": [],
      "source": [
        "DROP_CANDIDATES = [\n",
        "    \"ref\",\n",
        "    \"row\",\n",
        "    \"quarter\",\n",
        "    \"aggregate_year\",\n",
        "    \"aggregate_month\",\n",
        "    \"aggregate_day\",\n",
        "    \"aggregate_hour\",\n",
        "    \"period_over_which_rainfall_was_measured_days\",\n",
        "    \"days_of_accumulation_of_maximum_temperature\",\n",
        "    \"days_of_accumulation_of_minimum_temperature\",\n",
        "]\n",
        "\n",
        "COLUMN_ALIASES: Dict[str, Iterable[str]] = {\n",
        "    \"meter\": (\"meter_ui\", \"meter\", \"meter_id\"),\n",
        "    \"nmi\": (\"nmi_ui\", \"nmi_id\"),\n",
        "    \"delivered\": (\"delivered_value\", \"DELIVERED_VALUE\"),\n",
        "    \"daily_energy\": (\n",
        "        \"daily_energy_usage\",\n",
        "        \"DAILY_ENERGY_USAGE\",\n",
        "        \"Daily Energy Usage\",\n",
        "    ),\n",
        "    \"received\": (\"received_value\", \"RECEIVED_VALUE\"),\n",
        "    \"power_zero\": (\"power_zero\", \"POWER_ZERO\"),\n",
        "    \"daily_energy_zero\": (\"daily_energy_zero\", \"DAILY_ENERGY_ZERO\"),\n",
        "    \"error_check_hour\": (\"error_check_hour\", \"Error Check Hour\"),\n",
        "    \"error_check_day\": (\"error_check_day\", \"Error Check Day\"),\n",
        "    \"quarter\": (\"quarter\", \"Quarter\"),\n",
        "    \"aggregate_date\": (\"aggregate_date\", \"Aggregate Date\"),\n",
        "    \"date\": (\"date\", \"Date\"),\n",
        "    \"time\": (\"time\", \"Time\"),\n",
        "    \"timestamp\": (\"timestamp\", \"Timestamp\"),\n",
        "}\n",
        "\n",
        "\n",
        "def resolve_column(df: pd.DataFrame, *candidates: Optional[str]) -> Optional[str]:\n",
        "    lookup = {col.lower(): col for col in df.columns}\n",
        "    for candidate in candidates:\n",
        "        if candidate is None:\n",
        "            continue\n",
        "        column = lookup.get(candidate.lower())\n",
        "        if column is not None:\n",
        "            return column\n",
        "    return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _guess_dayfirst(series: pd.Series, sample_size: int = 20) -> bool:\n",
        "    sample = series.dropna()\n",
        "    if sample.empty:\n",
        "        return False\n",
        "    if pd.api.types.is_datetime64_any_dtype(sample):\n",
        "        return False\n",
        "    sample_str = sample.astype(str).str.strip()\n",
        "    sample_str = sample_str[sample_str != \"\"].head(sample_size)\n",
        "    if sample_str.empty:\n",
        "        return False\n",
        "    for value in sample_str:\n",
        "        tokens = [tok for tok in re.split(r\"[^0-9]\", value) if tok]\n",
        "        if len(tokens) >= 3:\n",
        "            first = tokens[0]\n",
        "            last = tokens[-1]\n",
        "            if len(last) == 4 and len(first) <= 2:\n",
        "                return True\n",
        "    return False\n",
        "def _optimize_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    opt = df.copy()\n",
        "\n",
        "    obj_cols = opt.select_dtypes(include=[\"object\", \"string\"]).columns\n",
        "    for col in obj_cols:\n",
        "        series = opt[col]\n",
        "        name = col.lower()\n",
        "\n",
        "        if \"date\" in name or \"timestamp\" in name:\n",
        "            dayfirst = _guess_dayfirst(series)\n",
        "            converted = pd.to_datetime(series, errors=\"coerce\", dayfirst=dayfirst)\n",
        "            if isinstance(converted.dtype, DatetimeTZDtype):\n",
        "                converted = converted.dt.tz_localize(None)\n",
        "            if converted.notna().mean() > 0.9:\n",
        "                opt[col] = converted\n",
        "                continue\n",
        "\n",
        "        numeric = pd.to_numeric(series, errors=\"coerce\")\n",
        "        if numeric.notna().mean() > 0.9:\n",
        "            if (numeric.dropna() % 1 == 0).all():\n",
        "                opt[col] = pd.to_numeric(series, errors=\"coerce\", downcast=\"integer\")\n",
        "            else:\n",
        "                opt[col] = pd.to_numeric(series, errors=\"coerce\", downcast=\"float\")\n",
        "            continue\n",
        "\n",
        "        nunique = series.nunique(dropna=True)\n",
        "        if nunique < 50 or (nunique / max(len(series), 1) < 0.5):\n",
        "            opt[col] = series.astype(\"category\")\n",
        "\n",
        "    int_cols = opt.select_dtypes(include=[\"int64\", \"int32\", \"Int64\", \"Int32\"]).columns\n",
        "    if len(int_cols) > 0:\n",
        "        opt[int_cols] = opt[int_cols].apply(pd.to_numeric, downcast=\"integer\")\n",
        "\n",
        "    float_cols = opt.select_dtypes(include=[\"float64\", \"Float64\"]).columns\n",
        "    if len(float_cols) > 0:\n",
        "        opt[float_cols] = opt[float_cols].apply(pd.to_numeric, downcast=\"float\")\n",
        "\n",
        "    num_cols = opt.select_dtypes(include=[np.number]).columns\n",
        "    for col in num_cols:\n",
        "        series = opt[col]\n",
        "        values = pd.unique(series.dropna())\n",
        "        if len(values) <= 2 and set(values).issubset({0, 1, 0.0, 1.0}):\n",
        "            try:\n",
        "                opt[col] = series.astype(\"boolean\") if series.isna().any() else series.astype(bool)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    return opt\n",
        "\n",
        "\n",
        "def load_dataset(data_path: Path, optimize: bool = True) -> pd.DataFrame:\n",
        "    if not data_path.exists():\n",
        "        raise FileNotFoundError(f\"Dataset not found at {data_path}\")\n",
        "\n",
        "    suffix = data_path.suffix.lower()\n",
        "    if suffix in {\".parquet\", \".pq\"}:\n",
        "        df = pd.read_parquet(data_path)\n",
        "    elif suffix == \".csv\":\n",
        "        dtype_map = {\n",
        "            \"nmi_ui\": \"category\",\n",
        "            \"meter_ui\": \"category\",\n",
        "            \"date\": \"string\",\n",
        "            \"time\": \"string\",\n",
        "            \"error_check_day\": \"float32\",\n",
        "            \"error_check_hour\": \"float32\",\n",
        "            \"delivered_value\": \"float32\",\n",
        "            \"daily_energy_usage\": \"float32\",\n",
        "            \"received_value\": \"float32\",\n",
        "            \"power_zero\": \"float32\",\n",
        "            \"daily_energy_zero\": \"float32\",\n",
        "            \"rainfall_amount_millimetres\": \"float32\",\n",
        "            \"period_over_which_rainfall_was_measured_days\": \"float32\",\n",
        "            \"maximum_temperature_degree_c\": \"float32\",\n",
        "            \"days_of_accumulation_of_maximum_temperature\": \"float32\",\n",
        "            \"minimum_temperature_degree_c\": \"float32\",\n",
        "            \"days_of_accumulation_of_minimum_temperature\": \"float32\",\n",
        "            \"daily_global_solar_exposure_mj_m_m\": \"float32\",\n",
        "        }\n",
        "        try:\n",
        "            df = pd.read_csv(\n",
        "                data_path,\n",
        "                dtype=dtype_map,\n",
        "                parse_dates=[\"aggregate_date\"],\n",
        "                na_values=[\"\", \" \"],\n",
        "            )\n",
        "        except ValueError:\n",
        "            df = pd.read_csv(\n",
        "                data_path,\n",
        "                dtype=dtype_map,\n",
        "                na_values=[\"\", \" \"],\n",
        "            )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file extension: {suffix}\")\n",
        "\n",
        "    df = df.copy()\n",
        "    df.columns = [col.strip() for col in df.columns]\n",
        "\n",
        "    if optimize:\n",
        "        df = _optimize_dtypes(df)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def ensure_timestamp_column(df: pd.DataFrame, target: str = \"timestamp\") -> Optional[str]:\n",
        "    timestamp_col = resolve_column(df, target)\n",
        "    if timestamp_col is not None:\n",
        "        dayfirst_guess = _guess_dayfirst(df[timestamp_col])\n",
        "        df[timestamp_col] = pd.to_datetime(df[timestamp_col], errors=\"coerce\", dayfirst=dayfirst_guess)\n",
        "        if isinstance(df[timestamp_col].dtype, DatetimeTZDtype):\n",
        "            df[timestamp_col] = df[timestamp_col].dt.tz_localize(None)\n",
        "        timestamp_col = timestamp_col\n",
        "    else:\n",
        "        date_col = resolve_column(df, \"date\", \"aggregate_date\")\n",
        "        time_col = resolve_column(df, \"time\")\n",
        "        if date_col and time_col:\n",
        "            date_str = df[date_col].astype(str).str.strip()\n",
        "            time_str = df[time_col].astype(str).str.strip()\n",
        "            parsed = pd.to_datetime(\n",
        "                date_str + \" \" + time_str,\n",
        "                format=\"%d/%m/%Y %H:%M\",\n",
        "                errors=\"coerce\",\n",
        "            )\n",
        "            df[target] = parsed\n",
        "            if isinstance(df[target].dtype, DatetimeTZDtype):\n",
        "                df[target] = df[target].dt.tz_localize(None)\n",
        "            timestamp_col = target\n",
        "        elif date_col:\n",
        "            dayfirst_guess = _guess_dayfirst(df[date_col])\n",
        "            df[target] = pd.to_datetime(\n",
        "                df[date_col],\n",
        "                errors=\"coerce\",\n",
        "                dayfirst=dayfirst_guess,\n",
        "            )\n",
        "            if isinstance(df[target].dtype, DatetimeTZDtype):\n",
        "                df[target] = df[target].dt.tz_localize(None)\n",
        "            timestamp_col = target\n",
        "        else:\n",
        "            timestamp_col = None\n",
        "\n",
        "    if timestamp_col:\n",
        "        agg_col = resolve_column(df, \"aggregate_date\")\n",
        "        if agg_col:\n",
        "            dayfirst_guess = _guess_dayfirst(df[agg_col])\n",
        "            agg_parsed = pd.to_datetime(\n",
        "                df[agg_col],\n",
        "                errors=\"coerce\",\n",
        "                dayfirst=dayfirst_guess,\n",
        "            )\n",
        "            if isinstance(agg_parsed.dtype, DatetimeTZDtype):\n",
        "                agg_parsed = agg_parsed.dt.tz_localize(None)\n",
        "            missing_mask = df[timestamp_col].isna()\n",
        "            if missing_mask.any():\n",
        "                df.loc[missing_mask, timestamp_col] = agg_parsed[missing_mask]\n",
        "\n",
        "        if isinstance(df[timestamp_col].dtype, DatetimeTZDtype):\n",
        "            df[timestamp_col] = df[timestamp_col].dt.tz_localize(None)\n",
        "\n",
        "    return timestamp_col\n",
        "\n",
        "\n",
        "def drop_known_columns(df: pd.DataFrame, protect: Optional[Iterable[str]] = None) -> pd.DataFrame:\n",
        "    protect_set = set(filter(None, protect or []))\n",
        "    to_drop: List[str] = []\n",
        "    for candidate in DROP_CANDIDATES:\n",
        "        column = resolve_column(df, candidate)\n",
        "        if column and column not in protect_set:\n",
        "            to_drop.append(column)\n",
        "    if to_drop:\n",
        "        df = df.drop(columns=sorted(set(to_drop)))\n",
        "    return df\n",
        "\n",
        "\n",
        "def identify_columns(df: pd.DataFrame) -> Dict[str, Optional[str]]:\n",
        "    return {key: resolve_column(df, *aliases) for key, aliases in COLUMN_ALIASES.items()}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c09ee76",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_meter_summaries(\n",
        "    df: pd.DataFrame,\n",
        "    meter_col: str,\n",
        "    timestamp_col: Optional[str] = None,\n",
        "    delivered_col: Optional[str] = None,\n",
        "    daily_energy_col: Optional[str] = None,\n",
        "    received_col: Optional[str] = None,\n",
        "    nmi_col: Optional[str] = None,\n",
        "    power_zero_col: Optional[str] = None,\n",
        "    daily_energy_zero_col: Optional[str] = None,\n",
        ") -> pd.DataFrame:\n",
        "    grouped = df.groupby(meter_col, observed=True)\n",
        "\n",
        "    named_aggs: Dict[str, pd.NamedAgg] = {}\n",
        "    if nmi_col:\n",
        "        named_aggs[\"nmi_count\"] = pd.NamedAgg(column=nmi_col, aggfunc=\"nunique\")\n",
        "    if timestamp_col:\n",
        "        named_aggs[\"start_time\"] = pd.NamedAgg(column=timestamp_col, aggfunc=\"min\")\n",
        "        named_aggs[\"end_time\"] = pd.NamedAgg(column=timestamp_col, aggfunc=\"max\")\n",
        "    if delivered_col:\n",
        "        named_aggs[\"observation_count\"] = pd.NamedAgg(column=delivered_col, aggfunc=\"count\")\n",
        "        named_aggs[\"delivered_mean\"] = pd.NamedAgg(column=delivered_col, aggfunc=\"mean\")\n",
        "        named_aggs[\"delivered_median\"] = pd.NamedAgg(column=delivered_col, aggfunc=\"median\")\n",
        "        named_aggs[\"delivered_std\"] = pd.NamedAgg(column=delivered_col, aggfunc=\"std\")\n",
        "        named_aggs[\"delivered_min\"] = pd.NamedAgg(column=delivered_col, aggfunc=\"min\")\n",
        "        named_aggs[\"delivered_p25\"] = pd.NamedAgg(column=delivered_col, aggfunc=lambda x: x.quantile(0.25))\n",
        "        named_aggs[\"delivered_p75\"] = pd.NamedAgg(column=delivered_col, aggfunc=lambda x: x.quantile(0.75))\n",
        "        named_aggs[\"delivered_max\"] = pd.NamedAgg(column=delivered_col, aggfunc=\"max\")\n",
        "        named_aggs[\"delivered_sum\"] = pd.NamedAgg(column=delivered_col, aggfunc=\"sum\")\n",
        "    if daily_energy_col:\n",
        "        named_aggs[\"daily_energy_mean\"] = pd.NamedAgg(column=daily_energy_col, aggfunc=\"mean\")\n",
        "        named_aggs[\"daily_energy_std\"] = pd.NamedAgg(column=daily_energy_col, aggfunc=\"std\")\n",
        "        named_aggs[\"daily_energy_sum\"] = pd.NamedAgg(column=daily_energy_col, aggfunc=\"sum\")\n",
        "    if received_col:\n",
        "        named_aggs[\"received_mean\"] = pd.NamedAgg(column=received_col, aggfunc=\"mean\")\n",
        "        named_aggs[\"received_sum\"] = pd.NamedAgg(column=received_col, aggfunc=\"sum\")\n",
        "    if power_zero_col:\n",
        "        named_aggs[\"power_zero_rate\"] = pd.NamedAgg(column=power_zero_col, aggfunc=\"mean\")\n",
        "    if daily_energy_zero_col:\n",
        "        named_aggs[\"daily_energy_zero_rate\"] = pd.NamedAgg(column=daily_energy_zero_col, aggfunc=\"mean\")\n",
        "\n",
        "    if not named_aggs:\n",
        "        raise ValueError(\"No aggregations defined. Verify that the expected columns are present.\")\n",
        "\n",
        "    summary = grouped.agg(**named_aggs)\n",
        "\n",
        "    if \"nmi_count\" in summary.columns:\n",
        "        summary[\"nmi_count\"] = summary[\"nmi_count\"].astype(\"int64\")\n",
        "    if \"observation_count\" in summary.columns:\n",
        "        summary[\"observation_count\"] = summary[\"observation_count\"].astype(\"int64\")\n",
        "\n",
        "    if (\n",
        "        timestamp_col\n",
        "        and \"start_time\" in summary.columns\n",
        "        and \"end_time\" in summary.columns\n",
        "        and \"observation_count\" in summary.columns\n",
        "    ):\n",
        "        summary[\"period_hours\"] = (\n",
        "            summary[\"end_time\"] - summary[\"start_time\"]\n",
        "        ).dt.total_seconds() / 3600.0 + 1\n",
        "        summary[\"period_days\"] = summary[\"period_hours\"] / 24.0\n",
        "        summary[\"coverage_pct\"] = (\n",
        "            summary[\"observation_count\"] / summary[\"period_hours\"].clip(lower=1.0) * 100.0\n",
        "        )\n",
        "\n",
        "    if delivered_col and \"delivered_std\" in summary.columns and \"delivered_mean\" in summary.columns:\n",
        "        summary[\"delivered_cv\"] = (\n",
        "            summary[\"delivered_std\"] / summary[\"delivered_mean\"]\n",
        "        ).replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    if \"power_zero_rate\" in summary.columns:\n",
        "        summary[\"power_zero_pct\"] = summary.pop(\"power_zero_rate\") * 100.0\n",
        "    if \"daily_energy_zero_rate\" in summary.columns:\n",
        "        summary[\"daily_energy_zero_pct\"] = summary.pop(\"daily_energy_zero_rate\") * 100.0\n",
        "\n",
        "    return summary.sort_index()\n",
        "\n",
        "\n",
        "def attach_peer_comparison(\n",
        "    df: pd.DataFrame,\n",
        "    summary: pd.DataFrame,\n",
        "    meter_col: str,\n",
        "    delivered_col: Optional[str] = None,\n",
        "    daily_energy_col: Optional[str] = None,\n",
        ") -> pd.DataFrame:\n",
        "    summary = summary.copy()\n",
        "\n",
        "    if delivered_col and delivered_col in df.columns and \"delivered_mean\" in summary.columns:\n",
        "        delivered_stats = df.groupby(meter_col, observed=True)[delivered_col].agg([\"sum\", \"count\"])\n",
        "        delivered_totals = delivered_stats[\"sum\"]\n",
        "        delivered_counts = delivered_stats[\"count\"]\n",
        "        all_sum = delivered_totals.sum()\n",
        "        all_count = delivered_counts.sum()\n",
        "\n",
        "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "            peer_mean = (all_sum - delivered_totals) / (all_count - delivered_counts)\n",
        "            peer_mean = peer_mean.replace({0.0: np.nan})\n",
        "            summary[\"delivered_vs_peers_pct\"] = (\n",
        "                (summary[\"delivered_mean\"] - peer_mean)\n",
        "                / peer_mean\n",
        "                * 100.0\n",
        "            )\n",
        "\n",
        "        if \"delivered_sum\" in summary.columns:\n",
        "            summary[\"delivered_mean_rank\"] = (\n",
        "                summary[\"delivered_mean\"].rank(ascending=False, method=\"dense\").astype(int)\n",
        "            )\n",
        "            summary[\"delivered_sum_rank\"] = (\n",
        "                summary[\"delivered_sum\"].rank(ascending=False, method=\"dense\").astype(int)\n",
        "            )\n",
        "\n",
        "    if daily_energy_col and daily_energy_col in df.columns and \"daily_energy_mean\" in summary.columns:\n",
        "        energy_stats = df.groupby(meter_col, observed=True)[daily_energy_col].agg([\"sum\", \"count\"])\n",
        "        energy_totals = energy_stats[\"sum\"]\n",
        "        energy_counts = energy_stats[\"count\"]\n",
        "        all_energy_sum = energy_totals.sum()\n",
        "        all_energy_count = energy_counts.sum()\n",
        "\n",
        "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "            peer_energy_mean = (all_energy_sum - energy_totals) / (all_energy_count - energy_counts)\n",
        "            peer_energy_mean = peer_energy_mean.replace({0.0: np.nan})\n",
        "            summary[\"daily_energy_vs_peers_pct\"] = (\n",
        "                (summary[\"daily_energy_mean\"] - peer_energy_mean)\n",
        "                / peer_energy_mean\n",
        "                * 100.0\n",
        "            )\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "def hourly_usage_profile(\n",
        "    df: pd.DataFrame,\n",
        "    meter_id: str,\n",
        "    meter_col: str,\n",
        "    timestamp_col: str,\n",
        "    value_col: str,\n",
        ") -> pd.DataFrame:\n",
        "    if timestamp_col not in df.columns:\n",
        "        raise ValueError(\"Timestamp column not present in dataframe\")\n",
        "    if meter_id not in set(df[meter_col].astype(str)):\n",
        "        raise ValueError(f\"Meter {meter_id} not present in dataset\")\n",
        "\n",
        "    df = df.copy()\n",
        "    df[meter_col] = df[meter_col].astype(str)\n",
        "    df[meter_col] = df[meter_col].astype(\"category\")\n",
        "    df[timestamp_col] = pd.to_datetime(df[timestamp_col], errors=\"coerce\")\n",
        "\n",
        "    meter_mask = df[meter_col] == str(meter_id)\n",
        "    hours = df[timestamp_col].dt.hour\n",
        "    meter_profile = (\n",
        "        df.loc[meter_mask]\n",
        "        .groupby(hours[meter_mask])[value_col]\n",
        "        .agg(meter_mean=\"mean\", meter_median=\"median\")\n",
        "    )\n",
        "    peers_profile = (\n",
        "        df.loc[~meter_mask]\n",
        "        .groupby(hours[~meter_mask])[value_col]\n",
        "        .mean()\n",
        "    )\n",
        "\n",
        "    profile = (\n",
        "        meter_profile.reindex(range(24))\n",
        "        .join(peers_profile.rename(\"peer_mean\"), how=\"left\")\n",
        "    )\n",
        "\n",
        "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "        profile[\"lift_pct\"] = (\n",
        "            (profile[\"meter_mean\"] - profile[\"peer_mean\"])\n",
        "            / profile[\"peer_mean\"]\n",
        "            * 100.0\n",
        "        )\n",
        "\n",
        "    profile.index.name = \"hour\"\n",
        "    return profile.reset_index()\n",
        "\n",
        "\n",
        "def monthly_usage_profile(\n",
        "    df: pd.DataFrame,\n",
        "    meter_id: str,\n",
        "    meter_col: str,\n",
        "    timestamp_col: str,\n",
        "    value_col: str,\n",
        ") -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df[meter_col] = df[meter_col].astype(str)\n",
        "    df[timestamp_col] = pd.to_datetime(df[timestamp_col], errors=\"coerce\")\n",
        "    df[\"month\"] = df[timestamp_col].dt.to_period(\"M\")\n",
        "\n",
        "    meter_mask = df[meter_col] == str(meter_id)\n",
        "    if not meter_mask.any():\n",
        "        raise ValueError(f\"Meter {meter_id} not present in dataset\")\n",
        "\n",
        "    meter_month = df.loc[meter_mask].groupby(\"month\")[value_col].agg(\n",
        "        meter_mean=\"mean\",\n",
        "        meter_median=\"median\",\n",
        "        meter_sum=\"sum\",\n",
        "    )\n",
        "    peers_month = df.loc[~meter_mask].groupby(\"month\")[value_col].agg(\n",
        "        peer_mean=\"mean\",\n",
        "        peer_median=\"median\",\n",
        "        peer_sum=\"sum\",\n",
        "    )\n",
        "\n",
        "    profile = meter_month.join(peers_month, how=\"left\")\n",
        "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "        profile[\"sum_contribution_pct\"] = (\n",
        "            profile[\"meter_sum\"]\n",
        "            / (profile[\"meter_sum\"] + profile[\"peer_sum\"])\n",
        "            * 100.0\n",
        "        )\n",
        "\n",
        "    return profile.reset_index()\n",
        "\n",
        "\n",
        "def _finalize_plot(fig: plt.Figure, output_path: Optional[Path], show: bool) -> Optional[Path]:\n",
        "    if output_path is not None:\n",
        "        fig.savefig(output_path, dpi=150, bbox_inches=\"tight\")\n",
        "    if show:\n",
        "        plt.show()\n",
        "    plt.close(fig)\n",
        "    return output_path\n",
        "\n",
        "\n",
        "def generate_summary_plots(\n",
        "    summary: pd.DataFrame,\n",
        "    output_dir: Optional[Path] = None,\n",
        "    show: bool = False,\n",
        "    top_n: int = 15,\n",
        ") -> List[Path]:\n",
        "    if summary.empty:\n",
        "        return []\n",
        "    if output_dir is not None:\n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    top_n = max(1, min(top_n, len(summary)))\n",
        "    saved: List[Path] = []\n",
        "\n",
        "    plot_definitions = [\n",
        "        (\n",
        "            \"delivered_mean\",\n",
        "            f\"Top {top_n} meters by average delivered value\",\n",
        "            \"Average delivered value (kWh)\",\n",
        "        ),\n",
        "        (\n",
        "            \"delivered_sum\",\n",
        "            f\"Top {top_n} meters by total delivered value\",\n",
        "            \"Total delivered value (kWh)\",\n",
        "        ),\n",
        "        (\n",
        "            \"power_zero_pct\",\n",
        "            \"Distribution of zero-power readings across meters\",\n",
        "            \"Power zero percentage\",\n",
        "        ),\n",
        "        (\n",
        "            \"coverage_pct\",\n",
        "            \"Coverage percentage per meter\",\n",
        "            \"Coverage percentage\",\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    for metric, title, xlabel in plot_definitions:\n",
        "        if metric not in summary.columns:\n",
        "            continue\n",
        "        if sns is None and metric not in {\"delivered_mean\", \"delivered_sum\"}:\n",
        "            continue\n",
        "\n",
        "        if metric in {\"delivered_mean\", \"delivered_sum\"}:\n",
        "            ordered = (\n",
        "                summary.dropna(subset=[metric])\n",
        "                .sort_values(metric, ascending=False)\n",
        "                .head(top_n)\n",
        "            )\n",
        "            if ordered.empty:\n",
        "                continue\n",
        "\n",
        "            fig, ax = plt.subplots(figsize=(10, 6))\n",
        "            meters = ordered.index.astype(str)\n",
        "            values = ordered[metric]\n",
        "            color = None\n",
        "            if sns is not None:\n",
        "                color = sns.color_palette(\"viridis\", len(meters))\n",
        "            ax.barh(meters, values, color=color)\n",
        "            ax.set_title(title)\n",
        "            ax.set_xlabel(xlabel)\n",
        "            ax.set_ylabel(\"Meter\")\n",
        "            ax.invert_yaxis()\n",
        "            for idx, value in enumerate(values):\n",
        "                ax.text(value, idx, f\" {value:,.2f}\", va=\"center\", ha=\"left\", fontsize=8)\n",
        "            fig.tight_layout()\n",
        "            output_path = output_dir / f\"{metric}_top_{top_n}.png\" if output_dir else None\n",
        "            saved_path = _finalize_plot(fig, output_path, show)\n",
        "            if saved_path is not None:\n",
        "                saved.append(saved_path)\n",
        "        else:\n",
        "            fig, ax = plt.subplots(figsize=(8, 5))\n",
        "            data = summary[metric].dropna()\n",
        "            if data.empty:\n",
        "                plt.close(fig)\n",
        "                continue\n",
        "            if sns is not None:\n",
        "                sns.histplot(data, bins=20, kde=True, ax=ax)\n",
        "            else:\n",
        "                ax.hist(data, bins=20, alpha=0.75, color=\"tab:blue\", edgecolor=\"black\")\n",
        "            ax.set_title(title)\n",
        "            ax.set_xlabel(xlabel)\n",
        "            ax.set_ylabel(\"Count\")\n",
        "            fig.tight_layout()\n",
        "            output_path = output_dir / f\"{metric}_distribution.png\" if output_dir else None\n",
        "            saved_path = _finalize_plot(fig, output_path, show)\n",
        "            if saved_path is not None:\n",
        "                saved.append(saved_path)\n",
        "\n",
        "    return saved\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07bda015",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b25b087",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = load_dataset(DATA_PATH)\n",
        "column_map = identify_columns(df)\n",
        "timestamp_col = ensure_timestamp_column(df)\n",
        "\n",
        "protect_columns = [\n",
        "    column_map.get(\"meter\"),\n",
        "    column_map.get(\"nmi\"),\n",
        "    timestamp_col,\n",
        "    column_map.get(\"aggregate_date\"),\n",
        "    column_map.get(\"date\"),\n",
        "    column_map.get(\"time\"),\n",
        "]\n",
        "df = drop_known_columns(df, protect=protect_columns)\n",
        "\n",
        "print(f\"Rows: {len(df):,} | Columns: {df.shape[1]}\")\n",
        "display(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numeric_columns = df.select_dtypes(include=[np.number, \"Float32\", \"Float64\", \"Int32\", \"Int64\", \"boolean\"]).columns.tolist()\n",
        "categorical_columns = df.select_dtypes(include=[\"category\", \"object\", \"string\"]).columns.tolist()\n",
        "\n",
        "print(f\"Numeric columns: {len(numeric_columns)}\")\n",
        "print(f\"Categorical columns: {len(categorical_columns)}\")\n",
        "\n",
        "column_map[\"delivered\"] = column_map.get(\"delivered\") or resolve_column(df, \"delivered_value\")\n",
        "column_map[\"daily_energy\"] = column_map.get(\"daily_energy\") or resolve_column(df, \"daily_energy_usage\")\n",
        "column_map[\"received\"] = column_map.get(\"received\") or resolve_column(df, \"received_value\")\n",
        "\n",
        "value_columns = [\n",
        "    column_map.get(\"delivered\"),\n",
        "    column_map.get(\"daily_energy\"),\n",
        "    column_map.get(\"received\"),\n",
        "]\n",
        "value_columns = [col for col in value_columns if col in df.columns]\n",
        "\n",
        "print(\"Target candidates:\", value_columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Global EDA (Former `data_loader.py` helpers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Top of data\")\n",
        "display(df.head())\n",
        "\n",
        "print(\"\n",
        "Data types\")\n",
        "df.info()\n",
        "\n",
        "print(\"\n",
        "Summary statistics (numeric)\")\n",
        "display(df.describe().T)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "missing_summary = (\n",
        "    df.isna().sum()\n",
        "    .to_frame(name=\"missing_count\")\n",
        "    .assign(missing_pct=lambda x: x[\"missing_count\"] / len(df) * 100)\n",
        "    .sort_values(\"missing_pct\", ascending=False)\n",
        ")\n",
        "print(\"Missing values (sorted)\")\n",
        "display(missing_summary.head(20))\n",
        "\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"Duplicate rows: {duplicate_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique_counts = df.nunique(dropna=True)\n",
        "print(\"Unique values per column (top 20)\")\n",
        "display(unique_counts.sort_values(ascending=False).head(20))\n",
        "\n",
        "error_hour_col = column_map.get(\"error_check_hour\")\n",
        "if error_hour_col and error_hour_col in df.columns:\n",
        "    print(f\"\n",
        "Unique values in '{error_hour_col}'\")\n",
        "    display(df[error_hour_col].value_counts(dropna=False))\n",
        "else:\n",
        "    print(\"\n",
        "No error check hour column found.\")\n",
        "\n",
        "error_day_col = column_map.get(\"error_check_day\")\n",
        "if error_day_col and error_day_col in df.columns:\n",
        "    print(f\"\n",
        "Unique values in '{error_day_col}'\")\n",
        "    display(df[error_day_col].value_counts(dropna=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "received_col = column_map.get(\"received\")\n",
        "if received_col and received_col in df.columns:\n",
        "    print(f\"Analyzing {received_col}\")\n",
        "    display(df[received_col].describe())\n",
        "    missing_count = df[received_col].isna().sum()\n",
        "    print(f\"Missing values: {missing_count} ({missing_count / len(df) * 100:.2f}%)\")\n",
        "    display(df[received_col].value_counts().head(10))\n",
        "else:\n",
        "    print(\"Received value column not found.\")\n",
        "\n",
        "if received_col and received_col in df.columns:\n",
        "    sample_value = df[received_col].dropna().unique()\n",
        "    if 0.001 in sample_value:\n",
        "        display(df.loc[df[received_col] == 0.001].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "delivered_col = column_map.get(\"delivered\")\n",
        "if delivered_col and delivered_col in df.columns:\n",
        "    print(f\"Analyzing {delivered_col}\")\n",
        "    stats = df[delivered_col].describe()\n",
        "    display(stats)\n",
        "    missing_count = df[delivered_col].isna().sum()\n",
        "    print(f\"Missing values: {missing_count} ({missing_count / len(df) * 100:.2f}%)\")\n",
        "\n",
        "    q25 = df[delivered_col].quantile(0.25)\n",
        "    q75 = df[delivered_col].quantile(0.75)\n",
        "    iqr = q75 - q25\n",
        "    lower = q25 - 1.5 * iqr\n",
        "    upper = q75 + 1.5 * iqr\n",
        "    outliers = df[(df[delivered_col] < lower) | (df[delivered_col] > upper)]\n",
        "    print(\"IQR bounds:\", lower, upper)\n",
        "    print(f\"Outliers detected: {len(outliers):,}\")\n",
        "else:\n",
        "    print(\"Delivered value column not found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if sns is not None and delivered_col and delivered_col in df.columns:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    sns.histplot(df[delivered_col].dropna(), bins=50, kde=True, ax=axes[0])\n",
        "    axes[0].set_title(f\"Distribution of {delivered_col}\")\n",
        "    axes[0].set_xlabel(delivered_col)\n",
        "    sns.boxplot(x=df[delivered_col], ax=axes[1])\n",
        "    axes[1].set_title(f\"Boxplot of {delivered_col}\")\n",
        "    plt.tight_layout()\n",
        "else:\n",
        "    print(\"Install seaborn to render histograms/boxplots.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "power_zero_col = column_map.get(\"power_zero\")\n",
        "if power_zero_col and power_zero_col in df.columns:\n",
        "    print(f\"Analyzing {power_zero_col}\")\n",
        "    display(df[power_zero_col].describe())\n",
        "    print(df[power_zero_col].value_counts(dropna=False))\n",
        "\n",
        "    if delivered_col and delivered_col in df.columns:\n",
        "        zero_deliveries = (df[delivered_col] == 0).sum()\n",
        "        print(f\"Zero delivered values: {zero_deliveries:,}\")\n",
        "\n",
        "energy_zero_col = column_map.get(\"daily_energy_zero\")\n",
        "energy_usage_col = column_map.get(\"daily_energy\")\n",
        "if energy_zero_col and energy_zero_col in df.columns:\n",
        "    print(f\"Analyzing {energy_zero_col}\")\n",
        "    display(df[energy_zero_col].describe())\n",
        "    print(df[energy_zero_col].value_counts(dropna=False))\n",
        "\n",
        "    if energy_usage_col and energy_usage_col in df.columns:\n",
        "        zero_energy = (df[energy_usage_col] == 0).sum()\n",
        "        print(f\"Zero daily energy usage values: {zero_energy:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if delivered_col and delivered_col in df.columns:\n",
        "    corr = df[numeric_columns].select_dtypes(include=[np.number]).corr()\n",
        "    if sns is not None:\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(corr, cmap=\"coolwarm\", center=0, annot=False)\n",
        "        plt.title(\"Correlation matrix (numeric features)\")\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Install seaborn to render correlation heatmap.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Per-Meter Profiling (From `per_meter_eda.py`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "meter_col = column_map.get(\"meter\")\n",
        "nmi_col = column_map.get(\"nmi\")\n",
        "\n",
        "if meter_col and timestamp_col and delivered_col:\n",
        "    meter_summary = compute_meter_summaries(\n",
        "        df,\n",
        "        meter_col=meter_col,\n",
        "        timestamp_col=timestamp_col,\n",
        "        delivered_col=delivered_col,\n",
        "        daily_energy_col=column_map.get(\"daily_energy\"),\n",
        "        received_col=column_map.get(\"received\"),\n",
        "        nmi_col=nmi_col,\n",
        "        power_zero_col=column_map.get(\"power_zero\"),\n",
        "        daily_energy_zero_col=column_map.get(\"daily_energy_zero\"),\n",
        "    )\n",
        "    meter_summary = attach_peer_comparison(\n",
        "        df,\n",
        "        meter_summary,\n",
        "        meter_col=meter_col,\n",
        "        delivered_col=delivered_col,\n",
        "        daily_energy_col=column_map.get(\"daily_energy\"),\n",
        "    )\n",
        "    print(\"Per-meter summary (top rows)\")\n",
        "    display(meter_summary.head())\n",
        "else:\n",
        "    meter_summary = pd.DataFrame()\n",
        "    print(\"Meter, timestamp, or delivered columns are missing; skipping per-meter summary.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not meter_summary.empty:\n",
        "    print(\"Summary snapshot\")\n",
        "    display(\n",
        "        meter_summary[\n",
        "            [\n",
        "                col\n",
        "                for col in [\n",
        "                    \"observation_count\",\n",
        "                    \"delivered_mean\",\n",
        "                    \"delivered_vs_peers_pct\",\n",
        "                    \"power_zero_pct\",\n",
        "                    \"coverage_pct\",\n",
        "                ]\n",
        "                if col in meter_summary.columns\n",
        "            ]\n",
        "        ]\n",
        "        .sort_values(by=\"delivered_mean\", ascending=False)\n",
        "        .head(10)\n",
        "    )\n",
        "else:\n",
        "    print(\"Meter summary is empty; nothing to display.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not meter_summary.empty:\n",
        "    _ = generate_summary_plots(meter_summary, show=True, top_n=10)\n",
        "else:\n",
        "    print(\"Meter summary is empty; skipping plots.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Intra-Meter Weekly Diagnostics\n",
        "\n",
        "This section extends the original scripts by examining how each meter behaves across weeks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if meter_col and timestamp_col and value_columns:\n",
        "    df_weekly = df[[meter_col, timestamp_col] + value_columns].copy()\n",
        "    df_weekly[timestamp_col] = pd.to_datetime(df_weekly[timestamp_col], errors=\"coerce\")\n",
        "    df_weekly = df_weekly.dropna(subset=[timestamp_col])\n",
        "    df_weekly[\"week_start\"] = df_weekly[timestamp_col].dt.to_period(\"W\").apply(lambda p: p.start_time)\n",
        "\n",
        "    weekly_stats_list = []\n",
        "    for value_col in value_columns:\n",
        "        stats = (\n",
        "            df_weekly.groupby([meter_col, \"week_start\"], observed=True)[value_col]\n",
        "            .agg(\n",
        "                count=\"count\",\n",
        "                mean=\"mean\",\n",
        "                median=\"median\",\n",
        "                std=\"std\",\n",
        "                min=\"min\",\n",
        "                q25=lambda x: x.quantile(0.25),\n",
        "                q75=lambda x: x.quantile(0.75),\n",
        "                max=\"max\",\n",
        "            )\n",
        "            .reset_index()\n",
        "        )\n",
        "        stats[\"value_column\"] = value_col\n",
        "        weekly_stats_list.append(stats)\n",
        "\n",
        "    weekly_stats = pd.concat(weekly_stats_list, ignore_index=True)\n",
        "    print(\"Weekly distribution statistics (sample)\")\n",
        "    display(weekly_stats.head(20))\n",
        "else:\n",
        "    weekly_stats = pd.DataFrame()\n",
        "    print(\"Cannot compute weekly diagnostics without meter, timestamp, and numeric value columns.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "if sns is None:\n",
        "    print(\"Install seaborn to render weekly diagnostics plots.\")\n",
        "elif weekly_stats.empty or meter_col is None or timestamp_col is None:\n",
        "    if weekly_stats.empty:\n",
        "        print(\"Weekly statistics not computed; skipping plots.\")\n",
        "    else:\n",
        "        print(\"Meter or timestamp columns missing; cannot build weekly diagnostics.\")\n",
        "else:\n",
        "    if not meter_summary.empty and \"delivered_sum\" in meter_summary.columns:\n",
        "        top_meters = (\n",
        "            meter_summary.sort_values(\"delivered_sum\", ascending=False)\n",
        "            .head(4)\n",
        "            .index.astype(str)\n",
        "        )\n",
        "    elif meter_col in df.columns:\n",
        "        top_meters = (\n",
        "            df[meter_col]\n",
        "            .astype(str)\n",
        "            .value_counts()\n",
        "            .head(4)\n",
        "            .index\n",
        "        )\n",
        "    else:\n",
        "        top_meters = []\n",
        "\n",
        "    if len(top_meters) == 0:\n",
        "        print(\"No meters available to plot.\")\n",
        "    else:\n",
        "        selected_meters = [str(m) for m in top_meters]\n",
        "\n",
        "        meter_subset = weekly_stats.copy()\n",
        "        meter_subset[meter_col] = meter_subset[meter_col].astype(str)\n",
        "        meter_subset = meter_subset[meter_subset[meter_col].isin(selected_meters)]\n",
        "        meter_subset[\"week_start\"] = pd.to_datetime(meter_subset[\"week_start\"], errors=\"coerce\")\n",
        "        meter_subset = meter_subset.dropna(subset=[\"week_start\"])\n",
        "        meter_subset[\"week_label\"] = meter_subset[\"week_start\"].dt.strftime(\"%Y-%m-%d\")\n",
        "        if {\"mean\", \"std\"}.issubset(meter_subset.columns):\n",
        "            meter_subset[\"cv\"] = (meter_subset[\"std\"] / meter_subset[\"mean\"]).replace([np.inf, -np.inf], np.nan)\n",
        "        else:\n",
        "            meter_subset[\"cv\"] = np.nan\n",
        "\n",
        "        df_hourly = df[[meter_col, timestamp_col] + value_columns].copy()\n",
        "        df_hourly = df_hourly[df_hourly[meter_col].astype(str).isin(selected_meters)]\n",
        "        df_hourly[timestamp_col] = pd.to_datetime(df_hourly[timestamp_col], errors=\"coerce\")\n",
        "        df_hourly = df_hourly.dropna(subset=[timestamp_col])\n",
        "        if df_hourly.empty:\n",
        "            print(\"No timestamped records available after filtering; skipping hourly diagnostics.\")\n",
        "        else:\n",
        "            df_hourly[meter_col] = df_hourly[meter_col].astype(str)\n",
        "            df_hourly[\"hour\"] = df_hourly[timestamp_col].dt.hour\n",
        "            df_hourly[\"week_start\"] = df_hourly[timestamp_col].dt.to_period(\"W\").apply(lambda p: p.start_time)\n",
        "            df_hourly[\"week_label\"] = df_hourly[\"week_start\"].dt.strftime(\"%Y-%m-%d\")\n",
        "            df_hourly[\"day\"] = df_hourly[timestamp_col].dt.date\n",
        "            df_hourly[\"day_of_week\"] = df_hourly[timestamp_col].dt.day_name()\n",
        "            hour_order = sorted(df_hourly[\"hour\"].unique())\n",
        "            week_order = sorted(df_hourly[\"week_label\"].unique())\n",
        "\n",
        "            daily_hour = (\n",
        "                df_hourly.groupby([meter_col, \"day\", \"hour\"], observed=True)[value_columns]\n",
        "                .mean()\n",
        "                .reset_index()\n",
        "            ) if value_columns else pd.DataFrame()\n",
        "            if not daily_hour.empty:\n",
        "                daily_hour[meter_col] = daily_hour[meter_col].astype(str)\n",
        "\n",
        "            for value_col in value_columns:\n",
        "                if value_col not in df_hourly.columns:\n",
        "                    continue\n",
        "\n",
        "                weekly_data = meter_subset[meter_subset[\"value_column\"] == value_col].copy()\n",
        "                weekly_data = weekly_data.sort_values([meter_col, \"week_start\"])\n",
        "\n",
        "                if not weekly_data.empty:\n",
        "                    plt.figure(figsize=(10, 4))\n",
        "                    sns.lineplot(\n",
        "                        data=weekly_data,\n",
        "                        x=\"week_start\",\n",
        "                        y=\"median\",\n",
        "                        hue=meter_col,\n",
        "                        marker=\"o\",\n",
        "                    )\n",
        "                    plt.title(f\"Weekly median {value_col}\")\n",
        "                    plt.xlabel(\"Week\")\n",
        "                    plt.ylabel(f\"Median {value_col}\")\n",
        "                    plt.xticks(rotation=45)\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "                    if weekly_data[\"cv\"].notna().any():\n",
        "                        plt.figure(figsize=(10, 4))\n",
        "                        sns.lineplot(\n",
        "                            data=weekly_data.dropna(subset=[\"cv\"]),\n",
        "                            x=\"week_start\",\n",
        "                            y=\"cv\",\n",
        "                            hue=meter_col,\n",
        "                            marker=\"o\",\n",
        "                        )\n",
        "                        plt.title(f\"Weekly coefficient of variation for {value_col}\")\n",
        "                        plt.xlabel(\"Week\")\n",
        "                        plt.ylabel(\"Coefficient of variation\")\n",
        "                        plt.xticks(rotation=45)\n",
        "                        plt.tight_layout()\n",
        "                        plt.show()\n",
        "\n",
        "                hourly_box = sns.catplot(\n",
        "                    data=df_hourly[[meter_col, \"hour\", value_col]].dropna(),\n",
        "                    x=\"hour\",\n",
        "                    y=value_col,\n",
        "                    col=meter_col,\n",
        "                    kind=\"box\",\n",
        "                    col_wrap=2,\n",
        "                    sharey=False,\n",
        "                    height=4,\n",
        "                    order=hour_order,\n",
        "                )\n",
        "                hourly_box.set_titles(\"{col_name}\")\n",
        "                hourly_box.set_axis_labels(\"Hour of day\", value_col)\n",
        "                hourly_box.fig.suptitle(f\"Hourly distribution for {value_col}\", y=1.03)\n",
        "                hourly_box.fig.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "                weekly_box = sns.catplot(\n",
        "                    data=df_hourly[[meter_col, \"week_label\", value_col]].dropna(),\n",
        "                    x=\"week_label\",\n",
        "                    y=value_col,\n",
        "                    col=meter_col,\n",
        "                    kind=\"box\",\n",
        "                    col_wrap=2,\n",
        "                    sharey=False,\n",
        "                    height=4,\n",
        "                    order=week_order,\n",
        "                )\n",
        "                weekly_box.set_titles(\"{col_name}\")\n",
        "                weekly_box.set_axis_labels(\"Week\", value_col)\n",
        "                for ax in weekly_box.axes.flatten():\n",
        "                    ax.tick_params(axis=\"x\", rotation=45)\n",
        "                weekly_box.fig.suptitle(f\"Weekly boxplots of hourly {value_col}\", y=1.03)\n",
        "                weekly_box.fig.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "                if not daily_hour.empty and value_col in daily_hour.columns:\n",
        "                    volatility_box = sns.catplot(\n",
        "                        data=daily_hour[[meter_col, \"hour\", value_col]].dropna(),\n",
        "                        x=\"hour\",\n",
        "                        y=value_col,\n",
        "                        col=meter_col,\n",
        "                        kind=\"box\",\n",
        "                        col_wrap=2,\n",
        "                        sharey=False,\n",
        "                        height=4,\n",
        "                        order=hour_order,\n",
        "                    )\n",
        "                    volatility_box.set_titles(\"{col_name}\")\n",
        "                    volatility_box.set_axis_labels(\"Hour of day\", f\"Daily mean {value_col}\")\n",
        "                    volatility_box.fig.suptitle(f\"Hourly volatility (daily means) for {value_col}\", y=1.03)\n",
        "                    volatility_box.fig.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "                focus_meter = selected_meters[0]\n",
        "                focus_subset = df_hourly[df_hourly[meter_col] == focus_meter]\n",
        "                if not focus_subset.empty:\n",
        "                    heatmap_data = (\n",
        "                        focus_subset.groupby([\"day_of_week\", \"hour\"])[value_col]\n",
        "                        .std()\n",
        "                        .unstack(fill_value=np.nan)\n",
        "                        .reindex(\n",
        "                            [\n",
        "                                \"Monday\",\n",
        "                                \"Tuesday\",\n",
        "                                \"Wednesday\",\n",
        "                                \"Thursday\",\n",
        "                                \"Friday\",\n",
        "                                \"Saturday\",\n",
        "                                \"Sunday\",\n",
        "                            ]\n",
        "                        )\n",
        "                    )\n",
        "                    plt.figure(figsize=(12, 4))\n",
        "                    sns.heatmap(\n",
        "                        heatmap_data,\n",
        "                        cmap=\"mako\",\n",
        "                        linewidths=0.5,\n",
        "                        linecolor=\"white\",\n",
        "                        cbar_kws={\"label\": \"Std dev\"},\n",
        "                    )\n",
        "                    plt.title(f\"Day vs hour volatility for {focus_meter} ({value_col})\")\n",
        "                    plt.xlabel(\"Hour of day\")\n",
        "                    plt.ylabel(\"Day of week\")\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "- Use the weekly diagnostics above to isolate anomalous weeks per meter.\n",
        "- Extend the per-meter section with hourly or monthly plots via `hourly_usage_profile` / `monthly_usage_profile` if deeper dives are required.\n",
        "- Consider exporting `meter_summary` or `weekly_stats` to CSV for further modelling.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}